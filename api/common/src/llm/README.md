# å¤§æ¨¡å‹è°ƒç”¨æ¨¡å— (LLM Module)

è¿™ä¸ªæ¨¡å—æä¾›äº†ç»Ÿä¸€çš„å¤§æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¤šç§ä¸»æµçš„å¤§æ¨¡å‹æä¾›å•†ï¼ŒåŒ…æ‹¬ OpenAIã€DeepSeekã€Google Geminiã€Anthropic Claude ç­‰ã€‚

## ğŸ¯ ä¸»è¦ç‰¹æ€§

### 1. å¤šæä¾›å•†æ”¯æŒ
- **OpenAI** (ChatGPT) - å®Œæ•´æ”¯æŒ
- **DeepSeek** - å…¼å®¹ OpenAI API æ ¼å¼
- **Google Gemini** - è‡ªå®šä¹‰ API æ ¼å¼é€‚é…
- **Anthropic Claude** - è‡ªå®šä¹‰ API æ ¼å¼é€‚é…
- **è‡ªå®šä¹‰æä¾›å•†** - æ”¯æŒä»»ä½•å…¼å®¹ OpenAI API çš„æœåŠ¡

### 2. ç»Ÿä¸€æ¥å£
- ç»Ÿä¸€çš„è¯·æ±‚/å“åº”æ ¼å¼
- è‡ªåŠ¨æ ¼å¼è½¬æ¢å’Œé€‚é…
- é€æ˜çš„é”™è¯¯å¤„ç†
- ä¸€è‡´çš„é…ç½®ç®¡ç†

### 3. é«˜çº§åŠŸèƒ½
- ä¼šè¯ç®¡ç†
- æµå¼è¾“å‡ºï¼ˆéƒ¨åˆ†æ”¯æŒï¼‰
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- é…é¢å’Œé€Ÿç‡é™åˆ¶å¤„ç†
- è¯¦ç»†çš„æ—¥å¿—è®°å½•

### 4. æ˜“ç”¨æ€§
- ç®€å•çš„é…ç½®æ¥å£
- æ„å»ºå™¨æ¨¡å¼æ”¯æŒ
- å¼‚æ­¥/å¹¶å‘æ”¯æŒ
- å®Œå–„çš„é”™è¯¯å¤„ç†

## ğŸ“¦ æ ¸å¿ƒç»„ä»¶

### ç±»å‹ç³»ç»Ÿ (`types.rs`)
```rust
// æä¾›å•†æšä¸¾
pub enum LlmProvider {
    OpenAI,
    DeepSeek, 
    Gemini,
    Claude,
    Custom(u32),
}

// èŠå¤©æ¶ˆæ¯
pub struct ChatMessage {
    pub role: MessageRole,
    pub content: String,
    // ...
}

// èŠå¤©å®Œæˆè¯·æ±‚
pub struct ChatCompletionRequest {
    pub model: String,
    pub messages: Vec<ChatMessage>,
    pub temperature: Option<f32>,
    // ...
}
```

### é…ç½®ç®¡ç† (`config.rs`)
```rust
// åŸºç¡€é…ç½®
let config = LlmConfig::openai("your-api-key")
    .with_default_model("gpt-3.5-turbo")
    .with_timeout(30)
    .with_retry(3, 1000)
    .with_logging(true);

// å¤šé…ç½®ç®¡ç†
let mut manager = LlmConfigManager::new();
manager.add_config("openai", openai_config)?;
manager.add_config("deepseek", deepseek_config)?;
manager.set_default("openai")?;
```

### å®¢æˆ·ç«¯æ¥å£ (`client.rs`)
```rust
// åˆ›å»ºå®¢æˆ·ç«¯
let client = LlmClient::new();

// æ·»åŠ é…ç½®
client.add_config("openai", config).await?;

// ç®€å•èŠå¤©
let response = client.chat("Hello", None).await?;

// å®Œæ•´èŠå¤©å®Œæˆ
let request = ChatCompletionRequest::simple(model, messages);
let response = client.chat_completion(request, None).await?;
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```rust
use common::llm::{LlmClient, LlmConfig, ChatMessage};

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºå®¢æˆ·ç«¯
    let client = LlmClient::new();
    
    // æ·»åŠ  OpenAI é…ç½®
    let config = LlmConfig::openai("your-api-key");
    client.add_config("openai", config).await?;
    
    // ç®€å•èŠå¤©
    let response = client.chat("ä½ å¥½", Some("openai")).await?;
    println!("å›å¤: {}", response);
    
    Ok(())
}
```

### 2. ä½¿ç”¨æ„å»ºå™¨

```rust
use common::llm::LlmClientBuilder;

let client = LlmClientBuilder::new()
    .with_openai("openai", "your-openai-key")
    .with_deepseek("deepseek", "your-deepseek-key")
    .with_default("openai")
    .build()
    .await?;

let response = client.chat("Hello", None).await?;
```

### 3. ä¼šè¯ç®¡ç†

```rust
// åˆ›å»ºä¼šè¯
let session = client.create_session().await;

// åœ¨ä¼šè¯ä¸­èŠå¤©
let response1 = client.chat_in_session(session.id, "ä½ å¥½", None).await?;
let response2 = client.chat_in_session(session.id, "è¯·ç»§ç»­", None).await?;

// è·å–ä¼šè¯å†å²
let session = client.get_session(session.id).await.unwrap();
println!("æ¶ˆæ¯æ•°é‡: {}", session.messages.len());
```

### 4. å¤šæä¾›å•†å¯¹æ¯”

```rust
let providers = vec!["openai", "deepseek", "claude"];
let question = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ";

for provider in providers {
    match client.chat(question, Some(provider)).await {
        Ok(response) => println!("{}: {}", provider, response),
        Err(e) => println!("{} å¤±è´¥: {}", provider, e),
    }
}
```

## âš™ï¸ é…ç½®é€‰é¡¹

### OpenAI é…ç½®
```rust
let config = LlmConfig::openai("your-api-key")
    .with_organization_id("org-xxx")
    .with_project_id("proj-xxx")
    .with_default_model("gpt-4")
    .with_timeout(60)
    .with_proxy("http://proxy:8080");
```

### DeepSeek é…ç½®
```rust
let config = LlmConfig::deepseek("your-api-key")
    .with_default_model("deepseek-chat")
    .with_timeout(30);
```

### Gemini é…ç½®
```rust
let config = LlmConfig::gemini("your-api-key")
    .with_default_model("gemini-pro")
    .with_timeout(30);
```

### Claude é…ç½®
```rust
let config = LlmConfig::claude("your-api-key")
    .with_default_model("claude-3-sonnet-20240229")
    .with_timeout(30);
```

### è‡ªå®šä¹‰æä¾›å•†
```rust
let config = LlmConfig::custom(
    "your-api-key",
    "https://api.example.com/v1",
    "custom-model"
)
.with_extra_header("Custom-Header", "value")
.with_timeout(30);
```

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. é”™è¯¯å¤„ç†

```rust
use common::llm::{LlmError, LlmResult};

match client.chat("Hello", None).await {
    Ok(response) => println!("æˆåŠŸ: {}", response),
    Err(LlmError::AuthenticationError(msg)) => println!("è®¤è¯å¤±è´¥: {}", msg),
    Err(LlmError::QuotaExceededError(msg)) => println!("é…é¢è¶…é™: {}", msg),
    Err(LlmError::NetworkError(e)) => println!("ç½‘ç»œé”™è¯¯: {}", e),
    Err(e) => println!("å…¶ä»–é”™è¯¯: {}", e),
}
```

### 2. é‡è¯•æœºåˆ¶

```rust
let config = LlmConfig::openai("your-api-key")
    .with_retry(5, 2000); // æœ€å¤šé‡è¯•5æ¬¡ï¼Œå»¶è¿Ÿ2ç§’

// å®¢æˆ·ç«¯ä¼šè‡ªåŠ¨å¤„ç†å¯é‡è¯•çš„é”™è¯¯
let response = client.chat("Hello", None).await?;
```

### 3. æµå¼è¾“å‡º

```rust
use futures::StreamExt;

let request = ChatCompletionRequest::simple(model, messages)
    .with_stream(true);

let mut stream = client.chat_completion_stream(request, None).await?;

while let Some(chunk) = stream.next().await {
    match chunk {
        Ok(chunk) => {
            if let Some(choice) = chunk.choices.first() {
                print!("{}", choice.delta.content);
            }
        }
        Err(e) => eprintln!("æµå¼é”™è¯¯: {}", e),
    }
}
```

### 4. å·¥å…·è°ƒç”¨ï¼ˆFunction Callingï¼‰

```rust
use common::llm::{Tool, Function};

let tool = Tool {
    tool_type: "function".to_string(),
    function: Function {
        name: "get_weather".to_string(),
        description: Some("è·å–å¤©æ°”ä¿¡æ¯".to_string()),
        parameters: Some(serde_json::json!({
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "åŸå¸‚åç§°"
                }
            },
            "required": ["location"]
        })),
    },
};

let request = ChatCompletionRequest::simple(model, messages)
    .with_tools(vec![tool]);

let response = client.chat_completion(request, None).await?;
```

## ğŸ” è°ƒè¯•å’Œç›‘æ§

### 1. å¯ç”¨æ—¥å¿—

```rust
// åœ¨é…ç½®ä¸­å¯ç”¨æ—¥å¿—
let config = LlmConfig::openai("your-api-key")
    .with_logging(true);

// æˆ–è€…åœ¨ç¯å¢ƒå˜é‡ä¸­è®¾ç½®
// RUST_LOG=debug cargo run
```

### 2. è·å–ç»Ÿè®¡ä¿¡æ¯

```rust
// ä¼šè¯ç»Ÿè®¡
let stats = client.get_session_stats().await;
println!("æ€»ä¼šè¯æ•°: {}", stats.get("total_sessions").unwrap_or(&0));
println!("æ€»æ¶ˆæ¯æ•°: {}", stats.get("total_messages").unwrap_or(&0));

// Token ä½¿ç”¨ç»Ÿè®¡
if let Some(usage) = response.usage {
    println!("Token ä½¿ç”¨: è¾“å…¥={}, è¾“å‡º={}, æ€»è®¡={}", 
        usage.prompt_tokens, usage.completion_tokens, usage.total_tokens);
}
```

### 3. éªŒè¯é…ç½®

```rust
// éªŒè¯ API å¯†é’¥
let is_valid = client.validate_api_key(Some("openai")).await?;
if !is_valid {
    println!("API å¯†é’¥æ— æ•ˆ");
}

// è·å–å¯ç”¨æ¨¡å‹
let models = client.list_models(Some("openai")).await?;
for model in models.data {
    println!("æ¨¡å‹: {}", model.id);
}
```

## ğŸ›¡ï¸ å®‰å…¨æ³¨æ„äº‹é¡¹

### 1. API å¯†é’¥ç®¡ç†
- ä¸è¦åœ¨ä»£ç ä¸­ç¡¬ç¼–ç  API å¯†é’¥
- ä½¿ç”¨ç¯å¢ƒå˜é‡æˆ–é…ç½®æ–‡ä»¶
- å®šæœŸè½®æ¢ API å¯†é’¥

```rust
use std::env;

let api_key = env::var("OPENAI_API_KEY")
    .expect("è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡");
```

### 2. ç½‘ç»œå®‰å…¨
- ä½¿ç”¨ HTTPS ç«¯ç‚¹
- é…ç½®é€‚å½“çš„ä»£ç†å’Œé˜²ç«å¢™
- éªŒè¯ SSL è¯ä¹¦

### 3. æ•°æ®éšç§
- ä¸è¦å‘é€æ•æ„Ÿä¿¡æ¯åˆ°å¤–éƒ¨ API
- è€ƒè™‘ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹
- éµå®ˆæ•°æ®ä¿æŠ¤æ³•è§„

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. è¿æ¥æ± 
```rust
// å®¢æˆ·ç«¯å†…éƒ¨ä½¿ç”¨è¿æ¥æ± ï¼Œæ”¯æŒå¹¶å‘è¯·æ±‚
let tasks: Vec<_> = (0..10)
    .map(|i| client.chat(format!("é—®é¢˜ {}", i), None))
    .collect();

let responses = futures::future::join_all(tasks).await;
```

### 2. ç¼“å­˜
```rust
// å¯ä»¥åœ¨åº”ç”¨å±‚å®ç°å“åº”ç¼“å­˜
use std::collections::HashMap;

let mut cache = HashMap::new();
let question = "ä»€ä¹ˆæ˜¯AIï¼Ÿ";

if let Some(cached) = cache.get(question) {
    println!("ç¼“å­˜å‘½ä¸­: {}", cached);
} else {
    let response = client.chat(question, None).await?;
    cache.insert(question.to_string(), response.clone());
    println!("æ–°å“åº”: {}", response);
}
```

### 3. æ‰¹é‡å¤„ç†
```rust
// å¯¹äºå¤šä¸ªç‹¬ç«‹çš„è¯·æ±‚ï¼Œå¯ä»¥å¹¶è¡Œå¤„ç†
let questions = vec!["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"];
let tasks: Vec<_> = questions
    .into_iter()
    .map(|q| client.chat(q, None))
    .collect();

let responses = futures::future::join_all(tasks).await;
```

## ğŸ§ª æµ‹è¯•

### 1. å•å…ƒæµ‹è¯•
```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_config_validation() {
        let config = LlmConfig::openai("test-key");
        assert!(config.validate().is_ok());
        
        let invalid_config = LlmConfig::openai("")
            .with_timeout(0);
        assert!(invalid_config.validate().is_err());
    }
}
```

### 2. é›†æˆæµ‹è¯•
```rust
#[tokio::test]
async fn test_chat_completion() {
    let client = LlmClient::new();
    let config = LlmConfig::openai(env::var("TEST_API_KEY").unwrap());
    client.add_config("test", config).await.unwrap();
    
    let response = client.chat("Hello", Some("test")).await.unwrap();
    assert!(!response.is_empty());
}
```

## ğŸ”„ æ‰©å±•æ–°æä¾›å•†

è¦æ·»åŠ æ–°çš„å¤§æ¨¡å‹æä¾›å•†ï¼Œéœ€è¦å®ç° `LlmProvider` traitï¼š

```rust
use async_trait::async_trait;
use super::providers::LlmProvider as LlmProviderTrait;

pub struct CustomProvider {
    config: LlmConfig,
    client: reqwest::Client,
}

#[async_trait]
impl LlmProviderTrait for CustomProvider {
    fn name(&self) -> &'static str {
        "Custom"
    }
    
    async fn chat_completion(&self, request: ChatCompletionRequest) -> LlmResult<ChatCompletionResponse> {
        // å®ç°èŠå¤©å®Œæˆé€»è¾‘
        todo!()
    }
    
    // å®ç°å…¶ä»–å¿…éœ€çš„æ–¹æ³•...
}
```

## ğŸ“š æ›´å¤šç¤ºä¾‹

æŸ¥çœ‹ `examples/llm_example.rs` è·å–å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š
- åŸºæœ¬èŠå¤©åŠŸèƒ½
- ä¼šè¯ç®¡ç†
- å¤šæä¾›å•†å¯¹æ¯”
- é”™è¯¯å¤„ç†
- é…ç½®ç®¡ç†

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è¿™ä¸ªæ¨¡å—ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚
